---
title: "Bayesian Inference without Probability Density Functions"
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: yes
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r setup, include=FALSE}
options(width = 100)
library(knitr)
opts_chunk$set(echo = TRUE)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
options(rcpp.cache.dir = getwd())
options(scipen = 5)
library(rstan)
library(ggplot2)
options(mc.cores = parallel::detectCores())
```

## Obligatory Disclosure

* Ben is an employee of Columbia University, which has received several research grants to develop Stan
* Ben is also a manager of GG Statistics LLC, which uses Stan for business
* According to Columbia University 
  [policy](https://research.columbia.edu/content/conflict-interest-and-research), any such employee who 
  has any equity stake in, a title (such as officer or director) with, or is expected to earn at least 
  $\$5,000.00$ per year from a private company is required to disclose these facts in presentations

<div style="float: left; width: 60%;">
<video width="500" height="250" controls>
  <source src="https://video.twimg.com/ext_tw_video/999106109523742720/pu/vid/640x360/ljdUoEqXji0ES_CV.mp4?tag=3" type="video/mp4">
Your browser does not support the video tag.
</video> 
</div>
<div style="float: right; width: 40%;">
```{r, echo = FALSE, message = FALSE, fig.height=3, fig.width=4.5, warning = FALSE}
pp2 <- cranlogs::cran_downloads(package = "rstan", from = "2015-01-01", to = Sys.Date())
library(ggplot2)
ggplot(pp2,aes(x = date, y = count)) +
  geom_smooth(show.legend = FALSE, se = FALSE) + scale_y_log10() +
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = as.numeric(as.Date("2018-05-20")), color = "red") +
  labs(x = 'Date', y = 'Daily downloads',
    title = 'RStan Daily Downloads from RStudio Mirror',
    caption = "Season 3, Episode 9 of Billions") +
  theme(plot.caption = element_text(color = "red", face = "italic"))
```
</div>

## Main Points

> - Motivating Assumption: Most applied statisticians, especially those without graduate degrees from
  statistics departments, cannot make good use of PDFs
> - PDFs are like complex numbers in that they both enable some elegant math but are
  far removed from the real world applied statisticians inhabit
> - Easiest to articulate probabilistic information through quantiles
> - Choosing a prior PDF and choosing a transformation are flip sides of a coin
> - Probability distributions defined by inverse CDFs also tend to be more flexible

## Bayes Rule

- If $X$ and $Y$ are defined on discrete sample spaces, Bayes' Rule is intuitive:
$$\Pr\left(y \mid x\right) = \frac{\Pr\left(y\right) \times \Pr\left(x \mid y\right)}
{\Pr\left(x\right)} = \frac{\Pr\left(y\right) \times \Pr\left(x \mid y\right)}
{\sum_{y \in \Omega_Y} \Pr\left(y\right) \Pr\left(x \mid y\right)}$$

> - If $X$ and $\theta$ are defined on continuous sample / parameter spaces, Bayes' Rule
  is less intuitive because it involves many Probability Density Functions (PDFs)
$$f\left(\theta \mid x\right) = \frac{f\left(\theta\right) \times f\left(x \mid \theta\right)}
{f\left(x\right)} = \frac{f\left(\theta\right) \times f\left(x \mid \theta\right)}
{\int_\Theta f\left(\theta\right) \times f\left(x \mid \theta\right) d\theta}$$

> - But Bayes' Rule can be re-written under a change-of-variables from $\theta$ to $p$
$$f\left(p \mid x\right) = \left|\frac{\partial}{\partial p}\theta\left(p\right) \right|
\frac{f\left(\theta\left(p\right)\right) \times f\left(x \mid \theta\left(p\right)\right)}
{f\left(x\right)} = \frac{f\left(p\right) f\left(x \mid \theta\left(p\right)\right)}{f\left(x\right)}$$

## Generative Models

- Generative modeling is more fundamental to Bayesianism than Bayes' Rule is
- Prior predictive matching is fairly intuitive even on
  continuous parameter spaces since it operates at the RNG level
  (where $\thicksim$ reads as "is drawn from"):
$$\widetilde{\theta} \thicksim \mathcal{Beta}\left(\mu, \phi\right);
  \widetilde{x} \thicksim \mathcal{Binomial}\left(n, \widetilde{\theta}\right)$$
  and then keep $\widetilde{\theta}$ iff $\widetilde{x} = x$. Acceptance
  proportion converges to $\Pr\left(x\right)$.

> - But in the Stan language, $\thicksim$ does NOT read as "is drawn from"
> - It could mean "would have been drawn from in the absence of observed data"
> - In practice, it causes the C++ to add a log-PDF value to the kernel
> - Most Stan users do not understand that, even if they have never used BUGS
> - BUGS users do not have to think about PDFs very much

## Problems with Common Probability Distributions

- There are too many probability distributions leading to a paradox of choice
- Many probability distributions were invented to describe the distribution of
  data under specific assumptions about the data-generating process
- Other probability distributions were invented to describe the distribution of
  a test statistic across datasets that were randomly sampled from a population
- Most common probability distributions have explicit expressions for $\mu$ and $\sigma^2$

    * Is there any random variable where you feel you know the variance without
      estimating it from a bunch of realizations?
    * How can you ask people to do non-elementary integrals in their heads?
      What if the prior expectation and / or variance does not even exist?
      
> - Historically, prior distribution families were chosen to facilitate Gibbs sampling. 
  Only recently has anyone [asked](http://www.metalogdistributions.com/publications.html)
  questions like "What probability distributions are most useful for expressing beliefs about unknowns?"

## Inverse Cumulative Distribution Functions (ICDFs)

- A CDF, $F\left(\theta \mid \dots\right)$, is an increasing function from 
  $\Theta$ to $\left[0,1\right]$ so its inverse always exists and is an increasing function from 
  $\left[0,1\right]$ to $\Theta$
- $F^{-1}\left(0.5 \mid \dots \right)$ is the median, while
  $F^{-1}\left(0.25 \mid \dots \right)$ and $F^{-1}\left(0.75 \mid \dots \right)$ are the lower and
  upper quartiles, so an ICDF is also called a quantile function
- ICDFs rarely have explicit forms and are not in the Stan language
- ICDFs are essentially never taught, even in graduate statistics classes
- Gilchrist (2000) is the only [textbook](https://www.google.com/books/edition/Statistical_Modelling_with_Quantile_Func/7c1LimP_e-AC?hl=en&gbpv=1&dq=Statistical+Modelling+with+Quantile+Functions&printsec=frontcover) 
dedicated to ICDFs
- If $\widetilde{p} \thicksim Uniform\left(0,1\right)$ and 
  $\widetilde{\theta} = F^{-1}\left(\widetilde{p} \mid \dots\right)$, 
  then $\widetilde{\theta}$ is a realization from a probability distribution that
  has the PDF $f\left(\theta \mid \dots\right) = \frac{\partial}{\partial \theta} F\left(\theta \mid \dots\right)$
- $\mathbb{E}\theta = \int_0^1 F^{-1}\left(p \mid \dots\right) dp = 
\int_\Theta \theta f\left(\theta \mid \dots\right) d\theta$ iff the integrals converge

## Cauchy Prior with(out) ICDF Reparameterization {.smaller}

<div class="columns-2">
```{r, cauchy, cache=TRUE, warning=FALSE, output.lines = 3:5, comment=""}
monitor(stan(model_code =
'
parameters {
  real theta;
}
model {
  theta ~ cauchy(0, 1);
}
', seed = 12345, refresh = 0))
```
WARNINGS ABOUT DIVERGENT TRANSITIONS $\uparrow$
```{r, cauchy_reparameterized, cache = TRUE, output.lines = 3:6, comment = ""}
monitor(stan(model_code =
'
parameters {
  real<lower = 0, upper = 1> p; // cumulative probability
}
transformed parameters {
  real theta = tan(pi() * (p - 0.5)); // ICDF of Cauchy
} // no model block needed because no observed data
', seed = 12345, refresh = 0))
```
$\uparrow$ No divergent transitions or warnings of any kind
</div>

> - We had thought of the Cauchy prior as desirable but
  unworkable in Stan without this ICDF "trick"
> - We need to start thinking about ICDF reparameterizations as being 
  good in and of themselves because they avoid the need for PDFs and 
  ultimately the need to think in terms of many common distributions

## Stan Skeleton with Inverse CDF Transformations

```{stan output.var="icdf", eval = FALSE}
data {
  int<lower = 0> N;                // number of observations
  vector[N] y;                     // observed outcomes
  ...                              // known hyperparameters
}
parameters {
  real<lower = 0, upper = 1> p;    // cumulative probability
}
transformed parameters {
  real theta = some_icdf(p, ...);  // parameter of interest
}
model {
  y ~ likelihood(theta);           // function of theta, not y
} // no explicit prior distribution for p
generated quantities {            
  real y_ = likelihood_rng(theta); // posterior predictive realization
}
```

## The No Name Distribution of the 1st Kind

$$\theta\left(p; \mathbf{c}\right) \equiv \sum_{k = 0}^K c_k T_k\left(2p - 1\right) = 
\sum_{k = 0}^K c_k \cos\left(k \cos^{-1}\left(2p - 1\right)\right)$$
where $T_k\left(2p - 1\right)$ is the $k$-th [shifted Chebyshev polynomial of the 1st kind](https://archive.siam.org/books/ot99/OT99SampleChapter.pdf)

* [Truncated Chebyshev series](http://www.chebfun.org/ATAP/) are used to _approximate_ a 
  Lipschitz-continuous function, but here we are using it to _define_ a quantile function 
  in much the same way that an entire function can be defined by its infinite Taylor series
* This construction for $\theta\left(p; \mathbf{c}\right)$ satisfies [Keelin and Powley's (2011)](http://www.metalogdistributions.com/images/KeelinPowley_QuantileParameterizedDistributions_2011.pdf) 
  definition of a "quantile-parameterized distribution" (QPD), provided that, for all $p \in \left(0,1\right)$,
  $$0 \leq \frac{\partial}{\partial p} \theta\left(p; \mathbf{c}\right) = 
  2\sum_{k = 1}^K k c_k U_{k - 1}\left(2p - 1\right) = 
  2\sum_{k = 1}^K k c_k \frac{\sin\left(k \cos^{-1}\left(2p - 1\right)\right)}
  {\sin\left(\cos^{-1}\left(2p - 1\right)\right)}$$
  where $U_k\left(2p - 1\right)$ is the $k$-th shifted Chebyshev polynomial of the 2nd kind
  
## Determining $c_k$

* Let $\mathbf{u}$ be an ordered vector of $K + 1$ given cumulative probabilities and let
  $$\underbrace{\mathbf{T}\left(\mathbf{u}\right)}_{\left(K + 1\right) \times \left(K + 1\right)} = 
  \begin{bmatrix} 
    1 & T_1\left(2u_0 - 1\right) & \dots & T_K\left(2u_0 - 1\right) \\
    1 & T_1\left(2u_1 - 1\right) & \dots & T_K\left(2u_1 - 1\right) \\
    \dots & \dots & \dots & \dots \\
    1 & T_1\left(2u_K - 1\right) & \dots & T_K\left(2u_K - 1\right)
  \end{bmatrix}$$
  
> - If a user inputs $\mathbf{u}$ and $\boldsymbol{\theta}\left(\mathbf{u}\right)$, we can solve for 
  $\mathbf{c} = \mathbf{T}\left(\mathbf{u}\right)^{-1}\boldsymbol{\theta}\left(\mathbf{u}\right)$ and 
  check that $\theta\left(p; \mathbf{c}\right)$ is non-decreasing. The user need not deal with $\mathbf{c}$ 
  or $T_k\left(2p - 1\right)$.
> - As $K \uparrow \infty$, $\theta\left(p; \mathbf{c}\right)$ can approach
  any Lipschitz-continuous inverse CDF

## Double Bounded Case

```{r}
source("no_name1.R"); q <- qno_name1(quantiles = c(0, 0.25, 1), p = c(0, 0.5, 1)) # ICDF
```
```{r, echo = FALSE, fig.width=11, fig.height=5, small.mar = TRUE}
curve(q(p), from = 0, to = 1, n = 10001, xname = "p", ylab = expression(theta), axes = FALSE)
p <- c(.25, .5, .75)
theta <- q(p)
axis(1, at = c(0, p, 1))
axis(2, at = c(0, p, 1), las = 1)
for (j in 2) {
  segments(x0 = p[j], y0 = -1000, y1 = theta[j], col = 2, lty = 2)
  segments(x0 = -1, y0 = theta[j], x1 = p[j],    col = 2, lty = 2)
}
```

## Overcoming Infeasibility {.build}

```{r, error = TRUE, fig.keep='none'}
q <- qno_name1(quantiles = c(0, 0.24, 1), p = c(0, 0.5, 1)) # Oh noes!
```
```{r}
q <- qno_name1(quantiles = c(0, 0.11, 0.24, 1), p = c(0, 0.25, 0.5, 1))
```
```{r, echo = FALSE, fig.width=11, fig.height=3, small.mar = TRUE}
curve(q(p), from = 0, to = 1, n = 10001, xname = "p", ylab = expression(theta), axes = FALSE)
p <- c(.25, .5)
theta <- q(p)
axis(1, at = c(0, p, 1))
axis(2, at = c(0, round(theta, digits = 3), 1), las = 1)
for (j in seq_along(p)) {
  segments(x0 = p[j], y0 = -1000, y1 = theta[j], col = 2, lty = 2)
  segments(x0 = -1, y0 = theta[j], x1 = p[j],    col = 2, lty = 2)
}
```

## Lower Bounded Case

* [Powley (2013)](http://www.metalogdistributions.com/images/Powley_Dissertation_2013-augmented.pdf)
  proves that a strictly increasing and infinitely differentiable transformation of a QPD is a QPD,
  but must choose transformations wisely
$$\theta\left(p; \mathbf{c}\right) \equiv e^{\tanh^{-1}\sum_{k = 0}^K c_k T_k\left(2p - 1\right)}
  \iff \tanh \log \theta\left(p; \mathbf{c}\right) \equiv \sum_{k = 0}^K c_k T_k\left(2p - 1\right)$$
  

```{r}
q <- qno_name1(quantiles = c(0, 1, Inf), p = c(0, 0.5, 1))
```
```{r, echo = FALSE, fig.width=11, fig.height=2.5, small.mar = TRUE, warning = FALSE}
curve(q(p), from = 0, to = 1, n = 10001, xname = "p", 
      ylab = expression(theta), axes = FALSE, log = "y")
p <- c(.25, .5, .75)
theta <- q(p)
axis(1, at = c(0, p, 1))
axis(2, las = 1)
for (j in 2) {
  segments(x0 = p[j], y0 = .Machine$double.eps, y1 = theta[j], col = 2, lty = 2)
  segments(x0 = -1, y0 = theta[j], x1 = p[j],    col = 2, lty = 2)
}
```

## Unbounded Case

$$\theta\left(p; \mathbf{c}\right) \equiv \tanh^{-1}\sum_{k = 0}^K c_k T_k\left(2p - 1\right)
  \iff \tanh \theta\left(p; \mathbf{c}\right) \equiv \sum_{k = 0}^K c_k T_k\left(2p - 1\right)$$
```{r}
q <- qno_name1(quantiles = c(-Inf, 0, Inf), p = c(0, 0.5, 1))
```
```{r, echo = FALSE, fig.width=11, fig.height=3.75, small.mar = TRUE}
curve(q(p), from = 0, to = 1, n = 10001, xname = "p", 
      ylab = expression(theta), axes = FALSE)
p <- c(.25, .5, .75)
theta <- q(p)
axis(1, at = c(0, p, 1))
axis(2, las = 1)
for (j in 2) {
  segments(x0 = p[j], y0 = -1000, y1 = theta[j], col = 2, lty = 2)
  segments(x0 = -1, y0 = theta[j], x1 = p[j],    col = 2, lty = 2)
}
```
  

## [Constant Elasticity of Substitution (CES) Models](https://cran.r-project.org/web/packages/micEconCES/vignettes/CES.pdf)

$$Y_{t} \approx \gamma e^{\lambda \left(t - 1\right)} \left(\delta\left(\delta_{1}K_{t}^{-\rho_{1}} + \left(1-\delta_{1}\right)E_{t}^{-\rho_{1}}\right)^{\frac{\rho}{\rho_1}} + \left(1-\delta\right)L_{t}^{-\rho}\right)^{-\frac{\nu}{\rho}}$$

- $Y_t$ is value added, $K_t$ is capital, $E_t$ is energy, and $L_t$ is labor in Kemfert (1988)
- $\rho = \frac{1}{\sigma} - 1$ and $\rho_1 = \frac{1}{\sigma_1} - 1$ where $\sigma > 0$ is
  the elasticity of substitution between labor and both capital and energy (the quantity of 
  interest), while $\sigma_1 > 0$ is the elasticity of substitution between capital
  and energy
- $\delta \in \left[0,1\right]$ and $\delta_1 \in \left[0,1\right]$ are input intensities
- $\nu > 0$ is the elasticity of scale with $\nu = 1$ indicating constant returns to scale
- $\gamma > 0$ is a less-interpretable total factor productivity parameter when $t = 1$
- $\lambda$ governs the time trend (a.k.a. drift)
- Take logarithms and assume Gaussian error with standard deviation $\omega > 0$.
  Informative priors on the parameters are essential to avoid divergences.

## Point [Estimates](https://www.econstor.eu/bitstream/10419/203136/1/main_SG.pdf) of Capital-Labor Substitutability

![elasticitiy estimates](elasticities.png)

## Prior Quantile Function for $\sigma$ and $\sigma_1$

```{r, fig.width=11, fig.height=4, small.mar = TRUE, warning = FALSE}
q <- qno_name1(quantiles = c(0, 0.27, 0.5, 0.9, Inf), p = c(0, 0.25, 0.5, 0.75, 1))
curve(q(p), from = 1e-16, to = 1, n = 1001, xname = "p", ylab = expression(sigma))
```

## Prior Quantile Function for $\delta$ and $\delta_1$

```{r, fig.width=11, fig.height=4, small.mar = TRUE}
q <- qno_name1(quantiles = c(0, 1 / 3, 0.5, 2 / 3, 1), p = c(0, 0.25, 0.5, 0.75, 1))
hist(q(runif(10000)), probability = TRUE, main = "", xlab = expression(delta))
```

## Stan Program for CES Model {.smaller}

<div class="columns-2">
```{stan CES, cache = TRUE, output.var="CES"}
// defines no_name1_{dlu}b_icdf(p, u, theta)
#include no_name1.stan
data {
  int<lower = 0> T; // if T == 0, this draws from priors
  vector[T] log_Y;
  vector[T] log_K;
  vector[T] log_E;
  vector[T] log_L;
  positive_ordered[5] u;    ordered[5] theta[7];
  positive_ordered[6] u_lg; ordered[6] theta_lg;
}
parameters {
  vector<lower = 0, upper = 1>[8] p;
}  // cumulative probability primitives
transformed parameters {
  real sigma   = no_name1_lb_icdf(p[1], u, theta[1]);
  real sigma_1 = no_name1_lb_icdf(p[2], u, theta[2]);
  real delta   = no_name1_db_icdf(p[3], u, theta[3]);
  real delta_1 = no_name1_db_icdf(p[4], u, theta[4]);
  real nu      = no_name1_lb_icdf(p[5], u, theta[5]);
  real omega   = no_name1_db_icdf(p[6], u, theta[6]);
  real lambda  = no_name1_db_icdf(p[7], u, theta[7]);
  real log_gamma = no_name1_db_icdf(p[8], u_lg, theta_lg);
}
model {
  real rho   = -1 + inv(sigma);
  real rho_1 = -1 + inv(sigma_1);
  real nu_rho = nu / rho;
  real log_delta = log(delta);
  real log_delta_1 = log(delta_1);
  real log1m_delta = log1m(delta);
  real log1m_delta_1 = log1m(delta_1);
  real rho_rho_1 = rho / rho_1;
  vector[T] mu;
  for (t in 1:T) // with numerical stability
    mu[t] = log_gamma
          + lambda * (t - 1)
          - nu_rho
          * log_sum_exp(log_delta + rho_rho_1
          * log_sum_exp(log_delta_1 - 
                        rho_1 * log_K[t],
                        log1m_delta_1 - 
                        rho_1 * log_E[t]),
                        log1m_delta - 
                        rho * log_L[t]);
  log_Y ~ normal(mu, omega); // log-likelihood
} // MLEs invariant to the ICDF transformations
```
</div>

## Preparing the `data` Block for the Previous Slide

```{r}
u <- c(0, 0.25, 0.5, 0.75, 1)
theta = list(sigma  = c(0, 0.27, 0.5, 0.9, Inf),  sigma_1 = c(0, 0.27, 0.5, 0.9, Inf),
             delta  = c(0, 1 / 3, 0.5, 2 / 3, 1), delta_1 = c(0, 1 / 3, 0.5, 2 / 3, 1),
             nu     = c(0, 0.6, 1.0, 1.4, Inf),   omega   = c(0, 0.016, 0.03, 0.05, 0.15),
             lambda = c(0, 0.01, 0.02, 0.03, 0.05))
u_lg = c(0, 0.25, 0.5, 0.75, 0.9, 1);
theta_lg = c(-2, 1, 3, 5, 7, 10)
```

## Maximum Likelihood Estimates of the CES Model

```{r}
data(GermanIndustry, package = "micEconCES")
GermanIndustry <- log(subset(GermanIndustry, year < 1973 | year > 1975)[ , 2:5])
colnames(GermanIndustry) <- paste0("log_", c('Y', 'K', 'L', 'E'))
data_list <- with(GermanIndustry, 
                  list(T = nrow(GermanIndustry), log_Y = log_Y, 
                       log_K = log_K, log_E = log_E, log_L = log_L,
                       u = u, theta = theta, u_lg = u_lg, theta_lg = theta_lg))
```
```{r}
MLEs <- optimizing(CES, data = data_list, as_vector = FALSE, refresh = 0, seed = 54321)
round(rbind(theta = unlist(MLEs$par[-1]), p = MLEs$par$p), digits = 3)
```

## Posterior Estimates for the CES Model

```{r, post, cache = TRUE, output.lines = 5:14, dependson = "CES"}
post <- sampling(CES, data = data_list, seed = 12345,
                 control = list(adapt_delta = 0.96, max_treedepth = 12), refresh = 0)
print(post, pars = "p", include = FALSE, probs = c(.025, .1, .25, .5, 0.75, .9, .975))
```
```{r, echo = FALSE}
cat("Maximized likelihood is", MLEs$value)
```

## Posterior Planes ($\sigma$ and $\sigma_1$ on log scale)

```{r, fig.width=11, fig.height=6, warning = FALSE, small.mar = TRUE, echo = FALSE}
pairs(post, pars = "p", include = FALSE, log = 1:2) # elasticities plotted on log scale
```

## Posterior Planes in $\log p$-space

```{r, fig.width=11, fig.height=6, warning = FALSE, small.mar = TRUE, echo = FALSE}
pairs(post, pars = names(MLEs$par[-1]), include = FALSE, log = TRUE)
```

## Conclusions

* Is your audience equipped to understand prior PDFs?
* If not, quantiles rather than expectations is an easier entry point to probability
* Can avoid prior PDFs by applying the logic of PRNGs that use an inverse CDF to transform a 
  standard uniform random variate into a random variate from the intended distribution
* There a few distributions like the GLD that have explicit inverse CDFs and (functions 
  of) quantiles as the hyperparameters
* We need to get inverse CDFs into Stan Math (many of them are in Boost) and the Stan language
* Copulas?

## Generalized Lambda Distribution (GLD)

The GLD lacks an explicit PDF / CDF so it is
[defined](https://mpra.ub.uni-muenchen.de/43333/3/MPRA_paper_43333.pdf)
by its inverse CDF:
$$F^{-1}\left(p \mid m, r, a, s\right) = 
m + r \times F^{-1}\left(p \mid m = 0, r = 1, a, s\right) \\
F^{-1}\left(p \mid m = 0, r = 1, a, s\right) = 
\frac{S\left(p; a, s\right) - S\left(0.5; a, s\right)}
{S\left(0.75; a, s\right) - S\left(0.25; a, s\right)} \\
S\left(p; a, s\right) = \frac{p^{\alpha + \beta} - 1}{\alpha + \beta} - 
\frac{\left(1 - p\right)^{\alpha - \beta} - 1}{\alpha - \beta},
\alpha = \frac{0.5 - s}{2\sqrt{s\left(1 - s\right)}},
\beta = \frac{a}{2\sqrt{1 - a^2}}$$

- $m$ is the median
- $r > 0$ is the interquartile range, i.e. the difference between the quartiles
- $a \in \left(-1,1\right)$ controls the asymmetry (if symmetric, then $a = 0$)
- $s \in \left(0,1\right)$ controls the steepness (i.e. the heaviness) of its tails
- Limits are needed as $s \rightarrow \frac{1}{2}\left(1 \pm a\right)$

## Special Cases of the GLD (for some $m$ and $r$)

```{r, echo = FALSE, fig.width=11, fig.height=5.5, fig.keep = c(1, 3, 5, 7, 9, 11, 13, 19, seq(22, 36, by = 2))}
new_slide = function(title = "Special Cases of the GLD (for some $m$ and $r$)") {
  knitr::asis_output(paste0("\n\n## ", title, "\n\n"))
}
par(mar = c(4, 4, .1, .1), las = 1, bg = "lightgrey")
plot(c(-1, 1), c(0,1), type = "n", las = 1, xlab = "Asymmetry (a)", ylab = "Steepness (s)")
new_slide()

polygon(x = c(-1, 0, 1), y = c(1, 1/2, 1), col = 2, border = 2)
text(x = 0, y = 1.02, labels = "Unbounded", col = 2)
new_slide()

polygon(x = c( 1,  1, 0), y = c(1, 0, 1 / 2), col = 5, border = 5)
text(x =  1.02, y = 1 / 2, labels = "Lower Bounded", col = 5, srt = 270)
new_slide()

polygon(x = c(-1, -1, 0), y = c(1, 0, 1 / 2), col = 4, border = 4)
text(x = -1.02, y = 1 / 2, labels = "Upper Bounded", col = 4, srt = 90)
new_slide()

polygon(x = c(-1, 0, 1), y = c(0, 1/2, 0), col = 3, border = 3)
text(x = 0, y = -0.02, labels = "Bounded on Both Sides", col = 3)
new_slide()

points(x = 0, y = 1 / 2, pch = 20)
text(x = 0, y = 1/2, labels = 'Logistic(0,1)', pos = 4)
new_slide()

points(x = 1, y = 0, pch = 20)
text(x = 1, y = -0.02, labels = 'Exponential(1)', pos = 4, srt = 90)
new_slide()

points(x = 0.412, y = 0.3, pch = 20)
text(x = 0.412, y = 0.3, labels = '"Gamma"(4,1)', pos = 4)

points(x = 0.6671, y = 0.1991, pch = 20)
text(x = 0.6671, y = 0.1991, labels = expression(paste('"', chi^2, '"(3)')), pos = 4)

points(x = 0.2844, y = 0.358, pch = 20)
text(x = 0.2844, y = 0.358, labels = '"Lognormal"(0,0.25)', pos = 4)
new_slide()

points(x = 0, y = 1 / 2 - 1 / sqrt(5), pch = 20)
points(x = 0, y = 1 / 2 - 2 / sqrt(17), pch = 20)
text(x = 0, y = 1 / 2 - 1 / sqrt(5), pos = 1, labels = 'Uniform(0,1)', offset = 1 / 5)
new_slide()

points(x = 0, y = 0.3661, pch = 20)
text(x = 0, y = 0.3661, labels = '"Normal"(0,1)', pos = 1)
new_slide()

points(x = 0, y = 0.647, pch = 20)
text(x = 0, y = 0.647, labels = '"Laplace"(0,1)', pos = 1)
new_slide()

points(x = 0, y = 0.9434, pch = 20)
text(x = 0, y = 0.9434, labels = '"Cauchy"(0,1)', pos = 3)
new_slide()

s <- function(a, k) {
H <- function(x) ifelse(x >= 0, 1, -1)
  1 / 2 - H(abs(a) - sqrt(4 / (4 + k^2))) * 
          sqrt( (1 - 2 * k * abs(1 / 2 * a / sqrt(1 - a^2)) + k^2 * (1 / 2 * a / sqrt(1 - a^2))^2) / 
                (4 - 8 * k * abs(1 / 2 * a / sqrt(1 - a^2)) + k^2 + 4 *  k^2 * (1 / 2 * a / sqrt(1 - a^2))^2) )
}
for (k in 1:4) {
  curve(s(a, k),from = -1, to = 1, xname = "a", add = TRUE, lty = 2, col = "gold")
  text(-0.5, s(0.5, k), labels = paste("k = ", k), col = "gold", pos = 1, offset = 2 / k)
  new_slide()
}
text(c(-0.46, -0.5), y = c(.425, .2), 
     labels = c("finite moments", "All moments finite"), col = "gold", pos = c(4,1))
```

## [Limitations of Generalized Lambda Distributions](https://xianblog.wordpress.com/2011/09/05/a-misleading-title/)

- The uniform distribution (and perhaps some others) is an exact special case
  of a GLD with $a = 0$ and either $s = 1 / 2 - 1 / \sqrt{5}$ or $s = 1 / 2 - 2 / \sqrt{17}$
  
    * But if you wanted to represent a uniform prior with a GLD, it would not
    matter which set of asymmetry and steepness hyperparameters you used

- There are some low-kurtosis distributions that no GLD can reach
  
    * Dudewicz and Karian (1996) [uses](http://personal.denison.edu/~karian/papers/ext_general_lambda_II.pdf)
    a generalization of the beta distribution to complement the GLD so that 4
    finite moments can always be matched
    
- The asymmetry and steepness parameters of a GLD are no more intuitive than
  the shape parameters of any other distribution 
  
    * But we can solve for them numerically with enough quantile information
    
- The only GLD that is consistent with the quantile information may be
  bounded on one or both sides even when the parameter space conceptually is not
  
    * But we can see when the posterior mass piles up on a boundary

## Double Bounded Case

```{r, GLD, warning = FALSE}
rstan::expose_stan_functions("quantile_functions.stan"); source("GLD_helpers.R")
(a_s <- GLD_solver_bounded(bounds = c(0, 1), median = 0.3, IQR = 0.4))
```
```{r, echo = FALSE, fig.width=11, fig.height=3.75, small.mar = TRUE}
m <- 0.3
r <- 0.4
curve(qgld(p, median = m, IQR = r, asymmetry = a_s[1], steepness = a_s[2]),
      from = 0, to = 1, n = 10001, xname = "p", ylab = expression(theta), axes = FALSE)
p <- c(.25, .5, .75)
theta <- qgld(p, median = m, IQR = r, a_s[1], a_s[2])
axis(1, at = c(0, p, 1))
axis(2, at = c(0, round(theta, digits = 3), 1), las = 1)
for (j in seq_along(p)) {
  segments(x0 = p[j], y0 = -1000, y1 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]),
           col = 2, lty = 2)
  segments(x0 = -1, y0 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]), x1 = p[j],
           col = 2, lty = 2)
}
```

## Lower Bounded Case

```{r}
(a_s <- GLD_solver_LBFGS(lower_quartile = 0.5, median = 1, upper_quartile = 1.75, 
                         other_quantile = 0, alpha = 0, check = FALSE))
```
```{r, echo = FALSE, fig.width=11, fig.height=3.75, small.mar = TRUE}
m <- 1
r <- 1.25
curve(qgld(p, median = m, IQR = r, asymmetry = a_s[1], steepness = a_s[2]),
      from = 0, to = 1, xname = "p", ylab = expression(theta), axes = FALSE)
p <- c(.25, .5, .75)
theta <- qgld(p, median = m, IQR = r, a_s[1], a_s[2])
axis(1, at = c(0, p, 1))
axis(2, at = c(0, round(theta, digits = 3), 6), las = 1)
for (j in seq_along(p)) {
  segments(x0 = p[j], y0 = -1000, y1 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]),
           col = 2, lty = 2)
  segments(x0 = -1, y0 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]), x1 = p[j],
           col = 2, lty = 2)
}
```

## Unbounded Case

```{r}
(a_s <- GLD_solver_LBFGS(lower_quartile = -1, median = 0, upper_quartile = 1, 
                         other_quantile = 3, alpha = 0.95, check = FALSE)) # "Laplace"(0,1)
```
```{r, echo = FALSE, fig.width=11, fig.height=3.5, small.mar = TRUE}
m <- 0
r <- 2
curve(qgld(p, median = m, IQR = r, asymmetry = a_s[1], steepness = a_s[2]),
      from = 0, to = 1, xname = "p", ylab = expression(theta), axes = FALSE)
p <- c(.25, .5, .75, .95)
theta <- qgld(p, median = m, IQR = r, a_s[1], a_s[2])
axis(1, at = c(0, p, 1))
axis(2, at = c(-5, round(theta, digits = 3), 5), las = 1)
for (j in seq_along(p)) {
  segments(x0 = p[j], y0 = -1000, y1 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]),
           col = 2, lty = 2)
  segments(x0 = -1, y0 = GLD_icdf(p[j], m, r, a_s[1], a_s[2]), x1 = p[j],
           col = 2, lty = 2)
}
```

## Accidentally Bounded Case

```{r}
(a_s <- GLD_solver_LBFGS(lower_quartile = -2/3, median = 0, upper_quartile = 2/3, 
                         other_quantile = 2, alpha = 0.98, check = FALSE))
GLD_icdf(0, median = 0, IQR = 4 / 3, asymmetry = a_s[1], steepness = a_s[2])
```
```{r, echo = FALSE, fig.width=10, fig.height=2.3, small.mar = TRUE}
m <- 0
r <- 4 / 3
curve(qgld(p, median = m, IQR = r, asymmetry = a_s[1], steepness = a_s[2]),
      from = 0, to = 1, xname = "p", ylab = expression(theta), n = 10001)
curve(qnorm(p), from = 0, to = 1, xname = "p", add = TRUE, col = 2, lty = 2, n = 10001)
legend("topleft", legend = c("GLD", "Standard Normal"), col = 1:2, lty = 1:2, box.lwd = NA)
```
